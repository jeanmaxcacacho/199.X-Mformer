{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRBbH+pH4fxrv6vyBRJcJ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanmaxcacacho/199.X-Mformer/blob/main/Mformer_replicate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODOs in this notebook:\n",
        "*   get sample data from our actual dataset\n",
        "*   instantiate all Mformer foundations\n",
        "*   obtain moral foundation measurements for all foundations"
      ],
      "metadata": {
        "id": "VcKZuod96SIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "FOUNDATIONS = [\n",
        "    \"authority\",\n",
        "    \"care\",\n",
        "    \"fairness\",\n",
        "    \"loyalty\",\n",
        "    \"sanctity\"\n",
        "]\n",
        "\n",
        "MODEL_FOUNDATIONS = []\n",
        "for F in FOUNDATIONS:\n",
        "  MODEL_FOUNDATIONS.append(f\"joshnguyen/mformer-{F}\")\n",
        "\n",
        "\"\"\"\n",
        "Pretrained models locked and loaded in MODELS as:\n",
        "(tokenizer, model)\n",
        "\n",
        "[0] - [4] respectively\n",
        "1. authority\n",
        "2. care\n",
        "3. fairness\n",
        "4. loyalty\n",
        "5. sanctity\n",
        "\"\"\"\n",
        "\n",
        "MODELS = []\n",
        "for MODEL_FOUNDATION in MODEL_FOUNDATIONS:\n",
        "  MODELS.append((\n",
        "      AutoTokenizer.from_pretrained(MODEL_FOUNDATION),\n",
        "      AutoModelForSequenceClassification.from_pretrained(\n",
        "          MODEL_FOUNDATION,\n",
        "          device_map=\"auto\"\n",
        "      )\n",
        "  ))"
      ],
      "metadata": {
        "id": "csAzjjAce-w0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load 5 texts from the dataset: https://www.kaggle.com/datasets/asaniczka/reddit-on-israel-palestine-daily-updated\n",
        "...then perform inference\n",
        "\n",
        "Sample texts were obtained arbitrarily from the preview table, these were not properly sampled!\n",
        "\"\"\"\n",
        "\n",
        "instances = [\n",
        "  \"who confiscates a small child's bike? nazis\",\n",
        "  \"Why aren't they deliberately hitting schools and hospitals? Amateurs!\",\n",
        "  \"What are you expecting? That the UN will declare a war on a country to save the sorry hide of hamas?\",\n",
        "  \"Superman says NO to genocide!\",\n",
        "  \"Only criminals hide their faces. Hope they het the pineapple treatment on hell\"\n",
        "]"
      ],
      "metadata": {
        "id": "9BpbiBTOe_VY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTHORITY\n",
        "authority_tokenizer = MODELS[0][0]\n",
        "authority_classifier = MODELS[0][1]\n",
        "\n",
        "authority_inputs = authority_tokenizer(\n",
        "    instances,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ").to(authority_classifier.device)\n",
        "\n",
        "authority_classifier.eval()\n",
        "with torch.no_grad():\n",
        "  authority_outputs = authority_classifier(**authority_inputs)\n",
        "\n",
        "authority_probs = torch.softmax(authority_outputs.logits, dim=1)\n",
        "authority_probs = authority_probs[:, 1]\n",
        "authority_probs = authority_probs.detach().cpu().numpy()\n",
        "\n",
        "# Print results\n",
        "print(f\"Probability of foundation 'AUTHORITY':\", \"\\n\")\n",
        "for instance, prob in zip(instances, authority_probs):\n",
        "    print(instance, \"->\", prob, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spzw45hvkT2w",
        "outputId": "5fb6a2cf-8c59-416f-c61e-3f99df6a0632"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of foundation 'AUTHORITY': \n",
            "\n",
            "who confiscates a small child's bike? nazis -> 0.3475795 \n",
            "\n",
            "Why aren't they deliberately hitting schools and hospitals? Amateurs! -> 0.2956997 \n",
            "\n",
            "What are you expecting? That the UN will declare a war on a country to save the sorry hide of hamas? -> 0.27424964 \n",
            "\n",
            "Superman says NO to genocide! -> 0.22523473 \n",
            "\n",
            "Only criminals hide their faces. Hope they het the pineapple treatment on hell -> 0.22267361 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmgm_SuenW8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}